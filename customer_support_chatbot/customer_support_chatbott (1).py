# -*- coding: utf-8 -*-
"""customer_support_chatbott.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15v9vEsXws10ECl4KA43muXgfR5DlEOBm
"""

!wget https://www.kaggle.com/api/v1/datasets/download/bitext/bitext-gen-ai-chatbot-customer-support-dataset

import zipfile
import os
filepath = '/content/bitext-gen-ai-chatbot-customer-support-dataset'
zip_ref = zipfile.ZipFile(filepath, 'r')
zip_ref.extractall('/content')
zip_ref.close()

import pandas as pd

df = pd.read_csv('/content/Bitext_Sample_Customer_Support_Training_Dataset_27K_responses-v11.csv')
display(df.head())
display(df.info())

display(df['category'].value_counts())
display(df['intent'].value_counts())

print("Shape before dropping duplicates:", df.shape)
df.drop_duplicates(inplace=True)
print("Shape after dropping duplicates:", df.shape)

df['instruction'] = df['instruction'].str.lower().str.replace(r'[^a-z0-9\s]', '', regex=True)
df['response'] = df['response'].str.lower().str.replace(r'[^a-z0-9\s]', '', regex=True)

display(df.head())

dialogflow_data = {}
for intent_name, group in df.groupby('intent'):
    dialogflow_data[intent_name] = {
        'training_phrases': group['instruction'].tolist(),
        'responses': group['response'].tolist()
    }

# Displaying a sample of the organized data
for intent, data in list(dialogflow_data.items())[:3]:
    print(f"Intent: {intent}")
    print(f"Training Phrases ({len(data['training_phrases'])}): {data['training_phrases'][:5]}...") # Display first 5 phrases
    print(f"Responses ({len(data['responses'])}): {data['responses'][:5]}...") # Display first 5 responses
    print("-" * 20)

from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
import numpy as np

instructions = df['instruction'].tolist()
intents = df['intent'].tolist()

# Tokenization
tokenizer = Tokenizer()
tokenizer.fit_on_texts(instructions)
sequences = tokenizer.texts_to_sequences(instructions)

# Padding
max_sequence_length = max([len(seq) for seq in sequences])
padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')

# Numericalization of labels
intent_to_label = {intent: i for i, intent in enumerate(sorted(list(set(intents))))}
labels = np.array([intent_to_label[intent] for intent in intents])
categorical_labels = to_categorical(labels, num_classes=len(intent_to_label))

# Splitting into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(padded_sequences, categorical_labels, test_size=0.2, random_state=42)

print(f"Shape of X_train: {X_train.shape}")
print(f"Shape of X_test: {X_test.shape}")
print(f"Shape of y_train: {y_train.shape}")
print(f"Shape of y_test: {y_test.shape}")
print(f"Max sequence length: {max_sequence_length}")
print(f"Number of unique intents: {len(intent_to_label)}")

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout

vocab_size = len(tokenizer.word_index) + 1
embedding_dim = 100
num_classes = len(intent_to_label)

model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))
model.add(LSTM(128, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(64))
model.add(Dropout(0.2))
model.add(Dense(num_classes, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.summary()

history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2)

import os
print(os.listdir('.'))

# Display the first few rows of the training data
print("Training Data:")
display(train_df.head())

loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f"Test Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")

# Examine the 'instruction' column for potential entities
print("Sample instructions with potential entities:")
for instruction in df['instruction'].sample(10, random_state=42):
    print(instruction)

# List potential entity types observed and provide examples
potential_entities = {
    "Order Number": ["order number", "purchase number", "orderid"],
    "Refund Amount": ["refund amount dollars", "currency symbol refund amount"],
    "Currency Symbol": ["currency symbol"],
    "Product Name": [], # No clear pattern in the samples for product names, but it's a common entity type
    "Date": [], # No clear pattern in the samples for dates, but it's a common entity type
    "Time": [], # No clear pattern in the samples for times, but it's a common entity type
    "Location": [] # No clear pattern in the samples for locations, but it's a common entity type
}

print("\nPotential Entity Types and Examples:")
for entity, examples in potential_entities.items():
    print(f"- {entity}: {examples if examples else 'No specific patterns observed in samples, but a likely entity type.'}")

# Acknowledge Dialogflow console limitation
print("\nNote: Defining and configuring these entities would be done within the Dialogflow console, which is not accessible in this environment.")

print("Configuring responses for the Dialogflow agent based on the 'response' data from the dataset is a crucial step in building a functional chatbot.")
print("However, this step is performed directly within the Dialogflow console.")
print("Due to the limitations of the current environment, we do not have access to the Dialogflow console, and therefore, we cannot complete this configuration step here.")
print("\nIn the Dialogflow console, you would typically navigate to each intent that has been created.")
print("Within the intent's configuration, there is a section for 'Responses'.")
print("Here, you would add the corresponding 'response' text from our dataset (`dialogflow_data` variable) as the agent's responses for that specific intent.")
print("The `dialogflow_data` variable, which we prepared earlier, contains the organized training phrases and responses for each intent and would serve as the data source for this manual configuration process in Dialogflow.")

print("The final step in setting up the Dialogflow agent is to train it.")
print("This process is initiated within the Dialogflow console to allow the agent to learn from the intents, training phrases, and entities that have been defined.")
print("\nSteps to train the Dialogflow agent in the console:")
print("1. Access the Dialogflow console for your agent.")
print("2. In the left-hand navigation panel, locate and click on the 'Train' section (sometimes found under 'Settings' or directly visible depending on the console version).")
print("3. Click on the 'Start Training' or 'Train' button.")
print("4. Dialogflow will then process the provided data. The training time can vary depending on the amount and complexity of the data.")
print("5. Monitor the training progress. Dialogflow will display the status (e.g., 'Training in progress', 'Training complete').")
print("6. Once training is complete, the agent is ready to be tested and integrated.")
print("\nSince we do not have access to the Dialogflow console in this environment, we cannot perform the actual training here.")

print("Exporting the trained Dialogflow agent allows you to back up your agent's configuration or import it into another Dialogflow environment.")
print("This action is performed directly within the Dialogflow console.")
print("\nSteps to export the Dialogflow agent in the console:")
print("1. Access the Dialogflow console for your agent.")
print("2. In the left-hand navigation panel, click on the gear icon (Settings) next to your agent's name.")
print("3. In the agent's settings, navigate to the 'Export and Import' tab.")
print("4. Under the 'Export' section, click on the 'Export as ZIP' button.")
print("5. A ZIP file containing your agent's configuration (including intents, entities, and responses) will be downloaded to your computer.")
print("\nThis ZIP file contains all the necessary information to recreate or transfer your Dialogflow agent.")
print("Since we do not have access to the Dialogflow console in this environment, we cannot perform the actual export here.")

print("Integrating a custom NLP model with Dialogflow primarily involves using webhooks.")
print("\n1. Webhook Integration:")
print("   - Dialogflow can be configured to send user queries to a webhook endpoint you provide.")
print("   - This webhook is a service (hosted on a server or cloud function) that contains your custom NLP model.")
print("   - When Dialogflow receives a user query, it triggers the webhook.")
print("   - The webhook receives the query, processes it using your custom NLP model to identify the intent and extract entities.")
print("   - Based on the model's output, the webhook constructs a response in a format Dialogflow understands.")
print("   - The webhook sends this response back to Dialogflow.")
print("   - Dialogflow then uses this response to formulate the final reply to the user.")
print("   - This allows you to leverage your more sophisticated or specialized NLP model for intent recognition and potentially generate dynamic responses based on external data or complex logic.")

print("\n2. Alternative Integration Methods (Briefly):")
print("   - Dialogflow API: You can use Dialogflow's API in your application.")
print("   - Your application sends the user query to your custom NLP model first.")
print("   - Based on your model's output, your application can then call the Dialogflow API to trigger a specific intent or directly provide a response to the user, bypassing Dialogflow's built-in NLP for that query.")
print("   - This method gives you more control over the flow but requires more logic in your application.")

print("\nLimitations in this environment:")
print("   - The actual implementation of a webhook or API integration requires setting up and deploying a server or cloud function to host the custom NLP model and handle communication with Dialogflow.")
print("   - This setup cannot be performed within the current notebook environment.")

print("Testing the integrated Dialogflow agent with your custom NLP model requires sending test queries to the Dialogflow agent.")
print("When the webhook is configured and pointing to your custom NLP model endpoint, the flow during testing is as follows:")
print("1. A user sends a query to the Dialogflow agent.")
print("2. Dialogflow receives the query and, based on your webhook configuration, sends it to your custom NLP model endpoint.")
print("3. Your custom NLP model processes the query, identifies the intent, extracts entities, and generates a response.")
print("4. The custom NLP model sends this response back to Dialogflow in the required format.")
print("5. Dialogflow uses the response from your custom model to formulate the final reply to the user.")

print("\nSince the custom NLP model and webhook are not deployed in this environment, we cannot perform actual end-to-end testing with live user queries here.")

print("\nIn a typical live environment, testing would be done through several methods:")
print("- Dialogflow Console Tester: The built-in tester in the Dialogflow console allows you to type in queries and see how the agent responds, including tracing the webhook calls.")
print("- Dialogflow API: You can send programmatic requests to your Dialogflow agent using the API and inspect the responses.")
print("- Client Application Integration: If your agent is integrated into a website, mobile app, or messaging platform, you can test it directly through that interface.")

print("\nIt is crucial to test with a wide variety of queries, including:")
print("- Different phrasing for the same intent.")
print("- Queries with variations in entity values.")
print("- Edge cases and unexpected inputs to ensure robustness.")
print("- Queries that might trigger fallback intents to refine handling of unknown inputs.")

print("Analyzing the model's test results...")
print(f"Test Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")

print("\nBased on the test accuracy and loss, we can assess the performance of the custom NLP model.")
print(f"A test accuracy of {accuracy:.4f} suggests that the model is performing very well on the unseen test data.")
print(f"A test loss of {loss:.4f} indicates a low error rate on the test data.")

print("\nGiven these strong performance metrics, significant refinement of the custom NLP model's architecture or hyperparameters may not be immediately necessary for this dataset.")
print("However, in a real-world scenario, if the performance was lower, we would consider:")
print("- Adjusting the number of LSTM layers or units.")
print("- Changing the dropout rates.")
print("- Experimenting with different optimizers or learning rates.")
print("- Increasing the number of epochs (with caution to avoid overfitting).")
print("- Exploring more advanced NLP techniques or model architectures (e.g., Transformers).")

print("\nRegarding the Dialogflow agent and integration logic:")
print("If testing in the Dialogflow console or through an integrated application revealed specific issues (e.g., misclassifications for certain phrases, failure to extract specific entities, or problems with the webhook communication), we would:")
print("- Add more diverse and representative training phrases to the relevant Dialogflow intents.")
print("- Refine or add new entity types and provide comprehensive examples in Dialogflow.")
print("- Review the webhook code for any errors in parsing Dialogflow requests or formatting responses.")
print("- Ensure the webhook endpoint is correctly configured in Dialogflow.")

print("\nSince we cannot perform live testing in this environment, we rely on the reported test metrics of the custom NLP model as an indicator of its potential performance.")
print("The high test accuracy suggests the model is capable of accurately classifying intents based on the provided data.")

# Commented out IPython magic to ensure Python compatibility.
# %pip install streamlit requests

!pip install tensorflow streamlit pyngrok -q

model.save('model.h5')
joblib.dump(tokenizer, 'tokenizer.pkl')
joblib.dump(dialogflow_data, 'dialogflow_data.pkl')
print("Model and data saved.")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import tensorflow as tf
# import numpy as np
# import joblib
# import random
# import re
# 
# # Load the model and data
# model = tf.keras.models.load_model('model.h5')
# with open('tokenizer.pkl', 'rb') as f:
#     tokenizer = joblib.load(f)
# with open('dialogflow_data.pkl', 'rb') as f:
#     dialogflow_data = joblib.load(f)
# 
# # Function to preprocess input and predict intent
# def predict_intent(text):
#     # Use re.sub() for regex replacement
#     cleaned_text = re.sub(r'[^a-z0-9\s]', '', text.lower())
#     sequence = tokenizer.texts_to_sequences([cleaned_text])
#     padded_sequence = tf.keras.preprocessing.sequence.pad_sequences(sequence, maxlen=69, padding='post')
#     prediction = model.predict(padded_sequence)
#     intent_index = np.argmax(prediction[0])
#     confidence = np.max(prediction[0])
#     intent_to_label = {i: intent for intent, i in enumerate(sorted(dialogflow_data.keys()))}
#     predicted_intent = [k for k, v in intent_to_label.items() if v == intent_index][0]
#     return predicted_intent, confidence
# 
# # Streamlit app
# st.title("Customer Support Chatbot")
# st.write("Ask about orders, payments, refunds, etc. (Powered by LSTM Model)")
# 
# if 'messages' not in st.session_state:
#     st.session_state.messages = []
# 
# for msg in st.session_state.messages:
#     with st.chat_message(msg["role"]):
#         st.markdown(msg["content"])
# 
# user_input = st.chat_input("Type your message here...")
# if user_input:
#     st.session_state.messages.append({"role": "user", "content": user_input})
#     with st.chat_message("user"):
#         st.markdown(user_input)
# 
#     # Predict intent
#     predicted_intent, confidence = predict_intent(user_input)
#     if confidence < 0.5:
#         bot_response = "I'm sorry, I didn't understand that. Can you please rephrase?"
#     else:
#         responses = dialogflow_data[predicted_intent]['responses']
#         bot_response = random.choice(responses) if responses else "No response available."
# 
#     st.session_state.messages.append({"role": "assistant", "content": bot_response})
#     with st.chat_message("assistant"):
#         st.markdown(bot_response)

from pyngrok import ngrok

!ngrok authtoken 323DE0bhyDi3Xf3HK6N5gr5E7V8_5ohpJSWAovNJYTVh6W9Pp # Replace with your ngrok authtoken
ngrok.kill()
tunnel = ngrok.connect(8501)
print("Streamlit app is live at:", tunnel.public_url)

!streamlit run app.py &> /dev/null &